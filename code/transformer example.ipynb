{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7feb0c3fe070>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "torch.manual_seed(3.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans  = nn.Transformer(nhead = 16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = trans(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8b03824cf0e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'src_mask' is not defined"
     ]
    }
   ],
   "source": [
    "output = trans(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c7b2550edc20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jade/AceSum/data/oposum/bag/train.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     for line in f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/3rd/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/3rd/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/3rd/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "with open ('/home/jade/AceSum/data/oposum/bag/train.json', 'r', encoding='UTF-8') as f:\n",
    "#     for line in f:\n",
    "    dic = json.loads(f.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "boolean_list = [False, False]\n",
    "\n",
    "# check if any element is true\n",
    "result = any(boolean_list)\n",
    "print(int(result))\n",
    "\n",
    "# Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/jade/yi_Data/Data/New_Data/Text_data/finishline/man_attribute.json', 'r') as f:\n",
    "    dic = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_n(dic, n):\n",
    "        i = 0\n",
    "        for k, v in dic.items():\n",
    "            i += 1\n",
    "            if i < n + 1:\n",
    "                print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good, but little slippery I like good design and price.\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(dic[\"-store-product-adidas-adilette-athletic-sandals-mens-sizing-prod21240001_styleId_F35416_colorId_001'\"][0]['text'].strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate 5\n",
      "recommend Yes\n",
      "text Good, but little slippery I like good design and price.\n",
      "attribute []\n"
     ]
    }
   ],
   "source": [
    "look_n(dic[\"-store-product-adidas-adilette-athletic-sandals-mens-sizing-prod21240001_styleId_F35416_colorId_001'\"][0], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"punct_chars\": None}\n",
    "# nlp.add_pipe(\"sentencizer\", config=config)\n",
    "\n",
    "\n",
    "# sentencizer = nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Construction from class\n",
    "from spacy.pipeline import Sentencizer\n",
    "sentencizer = Sentencizer()\n",
    "\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
    "assert len(list(doc.sents)) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-b7bc065a767b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-b7bc065a767b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(list(str(doc.sents))split())\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(list(str(doc.sents))split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[This is a sentence., This is another sentence.]\n"
     ]
    }
   ],
   "source": [
    "print(list(doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([0, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_dic = []\n",
    "with open ('/media/jade/yi_Data/Data/New_Data/Text_data/finishline/mil_man.json',  'r', encoding=\"UTF-8\") as f:\n",
    "    for line in f:\n",
    "        inst = json.loads(line)\n",
    "        man_dic.append(inst)\n",
    "print(len(man_dic), type(man_dic[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(man_dic))\n",
    "test_size = len(man_dic) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(man_dic, [train_size, test_size])\n",
    "print(type(test_dataset[0]))\n",
    "\n",
    "\n",
    "with open ('/media/jade/yi_Data/Data/New_Data/Text_data/finishline/dev_mil_man.json', 'w') as f:\n",
    "    for i in test_dataset:\n",
    "        f.write(json.dumps(i) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section figure out how to deal with the iterable dataset and how it's input, output looks like\n",
    "\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "# dataset = AspectDetectionDataset('/media/jade/yi_Data/Data/New_Data/Text_data/finishline/mil_man.json', tokenizer)\n",
    "dataset = AspectDetectionDataset('/home/jade/untextsum/data/finishline/mil_man.json', tokenizer)\n",
    "\n",
    "# dataset = AspectDetectionDataset('/media/jade/yi_Data/Data/text_summarization/space/space_train.json', tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=32, collate_fn=aspect_detection_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 77587.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (torch.Size([32, 9, 41]), torch.Size([32, 22]))\n",
      "1 (torch.Size([32, 10, 39]), torch.Size([32, 22]))\n",
      "2 (torch.Size([32, 11, 46]), torch.Size([32, 22]))\n",
      "3 (torch.Size([32, 12, 42]), torch.Size([32, 22]))\n",
      "4 (torch.Size([32, 8, 47]), torch.Size([32, 22]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t_o = 0\n",
    "for what, (inp_batch, out_batch) in enumerate(tqdm(list(dataloader))):\n",
    "    if t_o <= 4:\n",
    "        print(what, (inp_batch.size(), out_batch.size()))\n",
    "        t_o += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processor.readers import AspectDetectionDataset, aspect_detection_collate\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "from transformers import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import os\n",
    "# import readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-af878d64ef6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMIL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbest_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jade/untextsum/model/finishline/naive.10000.10.59.91.15.91.07'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'args'"
     ]
    }
   ],
   "source": [
    "from model.mil import MIL\n",
    "model = MIL()\n",
    "best_point = torch.load('/home/jade/untextsum/model/finishline/naive.10000.10.59.91.15.91.07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> Parameter containing:\n",
      "tensor([-1.9332e+17,  4.5800e-41, -1.9332e+17,  4.5800e-41],\n",
      "       requires_grad=True) (tensor([-1.9332e+17], grad_fn=<SplitBackward>), tensor([4.5800e-41], grad_fn=<SplitBackward>), tensor([-1.9332e+17], grad_fn=<SplitBackward>), tensor([4.5800e-41], grad_fn=<SplitBackward>)) tensor([[[-1.9332e+17]],\n",
      "\n",
      "        [[ 4.5800e-41]],\n",
      "\n",
      "        [[-1.9332e+17]],\n",
      "\n",
      "        [[ 4.5800e-41]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word_key = nn.Parameter(torch.Tensor(4))\n",
    "print(type(word_key), word_key, word_key.chunk(4, -1), word_key.view(4,-1).unsqueeze(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.s_mil import MIL\n",
    "\n",
    "def _update_counts(gold, pred, counts):\n",
    "    if gold * pred > 0:\n",
    "        counts[0] += 1\n",
    "    else:\n",
    "        counts[1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n"
     ]
    }
   ],
   "source": [
    "doc_counts = [[0] * 2] * 12\n",
    "print(doc_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_dir, num_aspects, model_name, load_model, train_file, dev_file, data_dir, model_dir, model_type, model_dim, num_heads,\n",
    "          vocab_size, batch_size, learning_rate, no_train_steps, no_warmup_steps, check_every, ckpt_every):\n",
    "    print('train the mil model you have following parameters:')\n",
    "    print('Preparing data...')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    dataset = AspectDetectionDataset(data_dir + '/' + dataset_dir + '/' + train_file, tokenizer)\n",
    "    print(data_dir + '/' + dataset_dir + '/' + train_file)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=aspect_detection_collate)\n",
    "    print(type(dataloader))\n",
    "    dev_dataset = AspectDetectionDataset(data_dir + '/' + dataset_dir + '/' + dev_file, tokenizer, shuffle=False)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=aspect_detection_collate)\n",
    "    print('Initializing model...')\n",
    "\n",
    "    model = MIL(model_type, model_dim, num_aspects, num_heads, no_warmup_steps)\n",
    "    model.cuda()\n",
    "\n",
    "  #optimizer = torch.optim.Adam(model.parameters())\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, no_warmup_steps, no_train_steps)\n",
    "    \n",
    "    step = 0\n",
    "    rng = np.random.default_rng()\n",
    "    if load_model is not None:\n",
    "        print('Loading model...')\n",
    "        best_point = torch.load(args.load_model)\n",
    "        model.load_state_dict(best_point['model'])\n",
    "        optimizer.load_state_dict(best_point['optimizer'])\n",
    "        scheduler.load_state_dict(best_point['scheduler'])\n",
    "        step = best_point['step']\n",
    "\n",
    "    print('Start training...')\n",
    "    while step < no_train_steps:\n",
    "        losses = []\n",
    "        for _, (inp_batch, out_batch) in enumerate(tqdm(list(dataloader))):\n",
    "            \n",
    "            model.train()\n",
    "            inp_batch = inp_batch.cuda()\n",
    "            out_batch = out_batch.cuda().float()\n",
    "            preds = model(inp_batch, out_batch, step=step)\n",
    "            document_pred = preds['document']\n",
    "            sentence_pred = preds['sentence']\n",
    "            loss = preds['loss']\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            step += 1\n",
    "            if step % check_every == 0:\n",
    "                print('Step %d Train Loss %.4f' % (step, np.mean(losses)))\n",
    "\n",
    "                doc_counts = [[0] * 2] * num_aspects\n",
    "                sent_counts = [[0] * 2] * num_aspects\n",
    "\n",
    "                dev_loss = []\n",
    "                for _, (inp_batch, out_batch) in enumerate(tqdm(list(dev_dataloader))):\n",
    "                    model.eval()\n",
    "\n",
    "                    inp_batch = inp_batch.cuda()\n",
    "#                     print(inp_batch.size())\n",
    "                    out_batch = out_batch.cuda().float()\n",
    "#                     print(out_batch.size(), len(out_batch))\n",
    "                    preds = model(inp_batch, out_batch)\n",
    "                    document_pred = preds['document']\n",
    "                    sentence_pred = preds['sentence']\n",
    "\n",
    "                    for bid in range(len(out_batch)):\n",
    "                        for aid in range(num_aspects):\n",
    "                            _update_counts(out_batch[bid][aid], document_pred[bid][aid], doc_counts[aid])\n",
    "\n",
    "                            for sid in range(len(sentence_pred[bid])):\n",
    "                                _update_counts(out_batch[bid][aid], sentence_pred[bid][sid][aid], sent_counts[aid])\n",
    "\n",
    "\n",
    "                    loss = preds['loss']\n",
    "                    dev_loss.append(loss.item())\n",
    "\n",
    "                print('Dev Loss %.4f' % np.mean(dev_loss))\n",
    "\n",
    "                doc_f1 = []\n",
    "                sent_f1 = []\n",
    "                for aid in range(num_aspects):\n",
    "                    doc_f1.append(2*doc_counts[aid][0] / float(2*doc_counts[aid][0] + doc_counts[aid][1]))\n",
    "                    sent_f1.append(2*sent_counts[aid][0] / float(2*sent_counts[aid][0] + sent_counts[aid][1]))\n",
    "                doc_f1 = np.mean(doc_f1) * 100\n",
    "                sent_f1 = np.mean(sent_f1) * 100\n",
    "\n",
    "                print('Document F1 %.4f' % doc_f1)\n",
    "                print('Sentence F1 %.4f' % sent_f1)\n",
    "\n",
    "                inp = inp_batch[0]\n",
    "                print('Document prediction', document_pred[0].tolist())\n",
    "                print('Gold', out_batch[0].tolist())\n",
    "                print()\n",
    "                for sid, sentence in enumerate(inp):\n",
    "                    sentence = tokenizer.decode(sentence, skip_special_tokens=True)\n",
    "                    if len(sentence.strip()) == 0:\n",
    "                        continue\n",
    "                    print('Sentence', sid, ':', sentence)\n",
    "                    print(sentence_pred[0][sid].tolist())\n",
    "                print('/n')\n",
    "\n",
    "\n",
    "            if step % ckpt_every == 0:\n",
    "                print('Saving...')\n",
    "                os.makedirs(model_dir + '/' + dataset_dir, exist_ok=True)\n",
    "                torch.save({\n",
    "                  'model': model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict(),\n",
    "                  'scheduler': scheduler.state_dict(),\n",
    "                  'step': step,\n",
    "                  'loss': np.mean(dev_loss)\n",
    "                }, model_dir + '/' + dataset_dir + '/' + model_name + '.%d.%.2f.%.2f.%.2f' % (step, np.mean(losses), doc_f1, sent_f1))\n",
    "                losses = []\n",
    "\n",
    "            if step == no_train_steps:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir ='finishline'\n",
    "num_aspects = 22\n",
    "model_name='naive'\n",
    "load_model=None\n",
    "train_file='mil_man.json'\n",
    "dev_file='dev_mil_man.json'\n",
    "data_dir='/home/jade/untextsum/data'\n",
    "model_dir='/home/jade/untextsum/model'\n",
    "model_type='distilroberta-base'\n",
    "model_dim=768\n",
    "num_heads=12\n",
    "vocab_size=50265\n",
    "batch_size=32\n",
    "learning_rate=1e-4\n",
    "no_train_steps=5000\n",
    "no_warmup_steps=10000\n",
    "check_every=100\n",
    "ckpt_every=1000\n",
    "\n",
    "# num_layers=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train the mil model you have following parameters:\n",
      "Preparing data...\n",
      "/home/jade/untextsum/data/finishline/mil_man.json\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Initializing model...\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/34 [00:00<00:05,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current data dimension is (32, 9, 41)\n",
      "first dimension transfer got torch.Size([288, 41])\n",
      "mask dimension transfer got torch.Size([288, 41])\n",
      "last word_vec dimension transfer got torch.Size([288, 41, 768])\n",
      "after drop step dimension transfer got torch.Size([288, 41, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([288, 41, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 9, 41, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([288, 41, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([288, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([288, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([288, 41])\n",
      "ah transform to a size is: torch.Size([32, 9, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([288, 41, 1])\n",
      "torch.Size([288, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([288, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([288, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([288, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([288, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([288, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([288, 41])\n",
      "ah transform to a size is: torch.Size([32, 9, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([288, 41, 1])\n",
      "torch.Size([288, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([288, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([288, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([288, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([288, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([288, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([288, 41])\n",
      "ah transform to a size is: torch.Size([32, 9, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([288, 41, 1])\n",
      "torch.Size([288, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([288, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([288, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([288, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 10, 39)\n",
      "first dimension transfer got torch.Size([320, 39])\n",
      "mask dimension transfer got torch.Size([320, 39])\n",
      "last word_vec dimension transfer got torch.Size([320, 39, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 3/34 [00:00<00:04,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop step dimension transfer got torch.Size([320, 39, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([320, 39, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 10, 39, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([320, 39, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([320, 39, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([320, 39])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([320, 39])\n",
      "ah transform to a size is: torch.Size([32, 10, 39])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([320, 39, 1])\n",
      "torch.Size([320, 39, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([320, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([320, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([320, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([320, 39, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([320, 39])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([320, 39])\n",
      "ah transform to a size is: torch.Size([32, 10, 39])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([320, 39, 1])\n",
      "torch.Size([320, 39, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([320, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([320, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([320, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([320, 39, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([320, 39])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([320, 39])\n",
      "ah transform to a size is: torch.Size([32, 10, 39])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([320, 39, 1])\n",
      "torch.Size([320, 39, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([320, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([320, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([320, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 11, 46)\n",
      "first dimension transfer got torch.Size([352, 46])\n",
      "mask dimension transfer got torch.Size([352, 46])\n",
      "last word_vec dimension transfer got torch.Size([352, 46, 768])\n",
      "after drop step dimension transfer got torch.Size([352, 46, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([352, 46, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 11, 46, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([352, 46, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 46, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 46])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 46])\n",
      "ah transform to a size is: torch.Size([32, 11, 46])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 46, 1])\n",
      "torch.Size([352, 46, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 46, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 46])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 46])\n",
      "ah transform to a size is: torch.Size([32, 11, 46])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 46, 1])\n",
      "torch.Size([352, 46, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 46, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 46])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 46])\n",
      "ah transform to a size is: torch.Size([32, 11, 46])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 46, 1])\n",
      "torch.Size([352, 46, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 12, 42)\n",
      "first dimension transfer got torch.Size([384, 42])\n",
      "mask dimension transfer got torch.Size([384, 42])\n",
      "last word_vec dimension transfer got torch.Size([384, 42, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 5/34 [00:00<00:04,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop step dimension transfer got torch.Size([384, 42, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([384, 42, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 12, 42, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([384, 42, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([384, 42, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([384, 42])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([384, 42])\n",
      "ah transform to a size is: torch.Size([32, 12, 42])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([384, 42, 1])\n",
      "torch.Size([384, 42, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([384, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([384, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([384, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([384, 42, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([384, 42])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([384, 42])\n",
      "ah transform to a size is: torch.Size([32, 12, 42])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([384, 42, 1])\n",
      "torch.Size([384, 42, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([384, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([384, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([384, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([384, 42, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([384, 42])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([384, 42])\n",
      "ah transform to a size is: torch.Size([32, 12, 42])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([384, 42, 1])\n",
      "torch.Size([384, 42, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([384, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([384, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([384, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 8, 47)\n",
      "first dimension transfer got torch.Size([256, 47])\n",
      "mask dimension transfer got torch.Size([256, 47])\n",
      "last word_vec dimension transfer got torch.Size([256, 47, 768])\n",
      "after drop step dimension transfer got torch.Size([256, 47, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([256, 47, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 8, 47, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([256, 47, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([256, 47, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([256, 47])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([256, 47])\n",
      "ah transform to a size is: torch.Size([32, 8, 47])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([256, 47, 1])\n",
      "torch.Size([256, 47, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([256, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([256, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([256, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([256, 47, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([256, 47])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([256, 47])\n",
      "ah transform to a size is: torch.Size([32, 8, 47])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([256, 47, 1])\n",
      "torch.Size([256, 47, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([256, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([256, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([256, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([256, 47, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([256, 47])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([256, 47])\n",
      "ah transform to a size is: torch.Size([32, 8, 47])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([256, 47, 1])\n",
      "torch.Size([256, 47, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([256, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([256, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([256, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 11, 39)\n",
      "first dimension transfer got torch.Size([352, 39])\n",
      "mask dimension transfer got torch.Size([352, 39])\n",
      "last word_vec dimension transfer got torch.Size([352, 39, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 7/34 [00:01<00:04,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop step dimension transfer got torch.Size([352, 39, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([352, 39, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 11, 39, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([352, 39, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 39, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 39])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 39])\n",
      "ah transform to a size is: torch.Size([32, 11, 39])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 39, 1])\n",
      "torch.Size([352, 39, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 39, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 39])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 39])\n",
      "ah transform to a size is: torch.Size([32, 11, 39])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 39, 1])\n",
      "torch.Size([352, 39, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 39, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 39])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 39])\n",
      "ah transform to a size is: torch.Size([32, 11, 39])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 39, 1])\n",
      "torch.Size([352, 39, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 13, 41)\n",
      "first dimension transfer got torch.Size([416, 41])\n",
      "mask dimension transfer got torch.Size([416, 41])\n",
      "last word_vec dimension transfer got torch.Size([416, 41, 768])\n",
      "after drop step dimension transfer got torch.Size([416, 41, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([416, 41, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 13, 41, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([416, 41, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([416, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([416, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([416, 41])\n",
      "ah transform to a size is: torch.Size([32, 13, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([416, 41, 1])\n",
      "torch.Size([416, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([416, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([416, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([416, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([416, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([416, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([416, 41])\n",
      "ah transform to a size is: torch.Size([32, 13, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([416, 41, 1])\n",
      "torch.Size([416, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([416, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([416, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([416, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([416, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([416, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([416, 41])\n",
      "ah transform to a size is: torch.Size([32, 13, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([416, 41, 1])\n",
      "torch.Size([416, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([416, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([416, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([416, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 14, 41)\n",
      "first dimension transfer got torch.Size([448, 41])\n",
      "mask dimension transfer got torch.Size([448, 41])\n",
      "last word_vec dimension transfer got torch.Size([448, 41, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 9/34 [00:01<00:03,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop step dimension transfer got torch.Size([448, 41, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([448, 41, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 14, 41, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([448, 41, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([448, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([448, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([448, 41])\n",
      "ah transform to a size is: torch.Size([32, 14, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([448, 41, 1])\n",
      "torch.Size([448, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([448, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([448, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([448, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([448, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([448, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([448, 41])\n",
      "ah transform to a size is: torch.Size([32, 14, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([448, 41, 1])\n",
      "torch.Size([448, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([448, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([448, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([448, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([448, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([448, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([448, 41])\n",
      "ah transform to a size is: torch.Size([32, 14, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([448, 41, 1])\n",
      "torch.Size([448, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([448, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([448, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([448, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 11, 43)\n",
      "first dimension transfer got torch.Size([352, 43])\n",
      "mask dimension transfer got torch.Size([352, 43])\n",
      "last word_vec dimension transfer got torch.Size([352, 43, 768])\n",
      "after drop step dimension transfer got torch.Size([352, 43, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([352, 43, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 11, 43, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([352, 43, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 43, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 43])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 43])\n",
      "ah transform to a size is: torch.Size([32, 11, 43])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 43, 1])\n",
      "torch.Size([352, 43, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 43, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 43])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 43])\n",
      "ah transform to a size is: torch.Size([32, 11, 43])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 43, 1])\n",
      "torch.Size([352, 43, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 43, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 43])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 43])\n",
      "ah transform to a size is: torch.Size([32, 11, 43])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 43, 1])\n",
      "torch.Size([352, 43, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 7, 42)\n",
      "first dimension transfer got torch.Size([224, 42])\n",
      "mask dimension transfer got torch.Size([224, 42])\n",
      "last word_vec dimension transfer got torch.Size([224, 42, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 11/34 [00:01<00:03,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop step dimension transfer got torch.Size([224, 42, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([224, 42, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 7, 42, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([224, 42, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([224, 42, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([224, 42])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([224, 42])\n",
      "ah transform to a size is: torch.Size([32, 7, 42])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([224, 42, 1])\n",
      "torch.Size([224, 42, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([224, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([224, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([224, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([224, 42, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([224, 42])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([224, 42])\n",
      "ah transform to a size is: torch.Size([32, 7, 42])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([224, 42, 1])\n",
      "torch.Size([224, 42, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([224, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([224, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([224, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([224, 42, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([224, 42])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([224, 42])\n",
      "ah transform to a size is: torch.Size([32, 7, 42])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([224, 42, 1])\n",
      "torch.Size([224, 42, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([224, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([224, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([224, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 10, 49)\n",
      "first dimension transfer got torch.Size([320, 49])\n",
      "mask dimension transfer got torch.Size([320, 49])\n",
      "last word_vec dimension transfer got torch.Size([320, 49, 768])\n",
      "after drop step dimension transfer got torch.Size([320, 49, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([320, 49, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 10, 49, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([320, 49, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([320, 49, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([320, 49])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([320, 49])\n",
      "ah transform to a size is: torch.Size([32, 10, 49])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([320, 49, 1])\n",
      "torch.Size([320, 49, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([320, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([320, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([320, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([320, 49, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([320, 49])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([320, 49])\n",
      "ah transform to a size is: torch.Size([32, 10, 49])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([320, 49, 1])\n",
      "torch.Size([320, 49, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([320, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([320, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([320, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([320, 49, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([320, 49])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([320, 49])\n",
      "ah transform to a size is: torch.Size([32, 10, 49])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([320, 49, 1])\n",
      "torch.Size([320, 49, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([320, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([320, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([320, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 8, 42)\n",
      "first dimension transfer got torch.Size([256, 42])\n",
      "mask dimension transfer got torch.Size([256, 42])\n",
      "last word_vec dimension transfer got torch.Size([256, 42, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 13/34 [00:01<00:03,  6.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop step dimension transfer got torch.Size([256, 42, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([256, 42, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 8, 42, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([256, 42, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([256, 42, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([256, 42])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([256, 42])\n",
      "ah transform to a size is: torch.Size([32, 8, 42])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([256, 42, 1])\n",
      "torch.Size([256, 42, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([256, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([256, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([256, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([256, 42, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([256, 42])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([256, 42])\n",
      "ah transform to a size is: torch.Size([32, 8, 42])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([256, 42, 1])\n",
      "torch.Size([256, 42, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([256, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([256, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([256, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([256, 42, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([256, 42])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([256, 42])\n",
      "ah transform to a size is: torch.Size([32, 8, 42])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([256, 42, 1])\n",
      "torch.Size([256, 42, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([256, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([256, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([256, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 12, 44)\n",
      "first dimension transfer got torch.Size([384, 44])\n",
      "mask dimension transfer got torch.Size([384, 44])\n",
      "last word_vec dimension transfer got torch.Size([384, 44, 768])\n",
      "after drop step dimension transfer got torch.Size([384, 44, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([384, 44, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 12, 44, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([384, 44, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([384, 44, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([384, 44])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([384, 44])\n",
      "ah transform to a size is: torch.Size([32, 12, 44])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([384, 44, 1])\n",
      "torch.Size([384, 44, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([384, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([384, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([384, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([384, 44, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([384, 44])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([384, 44])\n",
      "ah transform to a size is: torch.Size([32, 12, 44])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([384, 44, 1])\n",
      "torch.Size([384, 44, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([384, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([384, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([384, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([384, 44, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([384, 44])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([384, 44])\n",
      "ah transform to a size is: torch.Size([32, 12, 44])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([384, 44, 1])\n",
      "torch.Size([384, 44, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([384, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([384, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([384, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 11, 39)\n",
      "first dimension transfer got torch.Size([352, 39])\n",
      "mask dimension transfer got torch.Size([352, 39])\n",
      "last word_vec dimension transfer got torch.Size([352, 39, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 14/34 [00:02<00:02,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop step dimension transfer got torch.Size([352, 39, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([352, 39, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 11, 39, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([352, 39, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 39, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 39])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 39])\n",
      "ah transform to a size is: torch.Size([32, 11, 39])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 39, 1])\n",
      "torch.Size([352, 39, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 39, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 39])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 39])\n",
      "ah transform to a size is: torch.Size([32, 11, 39])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 39, 1])\n",
      "torch.Size([352, 39, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 39, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 39])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 39])\n",
      "ah transform to a size is: torch.Size([32, 11, 39])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 39, 1])\n",
      "torch.Size([352, 39, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 18, 43)\n",
      "first dimension transfer got torch.Size([576, 43])\n",
      "mask dimension transfer got torch.Size([576, 43])\n",
      "last word_vec dimension transfer got torch.Size([576, 43, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 15/34 [00:02<00:03,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop step dimension transfer got torch.Size([576, 43, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([576, 43, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 18, 43, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([576, 43, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([576, 43, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([576, 43])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([576, 43])\n",
      "ah transform to a size is: torch.Size([32, 18, 43])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([576, 43, 1])\n",
      "torch.Size([576, 43, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([576, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([576, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([576, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([576, 43, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([576, 43])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([576, 43])\n",
      "ah transform to a size is: torch.Size([32, 18, 43])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([576, 43, 1])\n",
      "torch.Size([576, 43, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([576, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([576, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([576, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([576, 43, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([576, 43])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([576, 43])\n",
      "ah transform to a size is: torch.Size([32, 18, 43])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([576, 43, 1])\n",
      "torch.Size([576, 43, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([576, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([576, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([576, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 17, 40)\n",
      "first dimension transfer got torch.Size([544, 40])\n",
      "mask dimension transfer got torch.Size([544, 40])\n",
      "last word_vec dimension transfer got torch.Size([544, 40, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 17/34 [00:02<00:03,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop step dimension transfer got torch.Size([544, 40, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([544, 40, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 17, 40, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([544, 40, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([544, 40, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([544, 40])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([544, 40])\n",
      "ah transform to a size is: torch.Size([32, 17, 40])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([544, 40, 1])\n",
      "torch.Size([544, 40, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([544, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([544, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([544, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([544, 40, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([544, 40])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([544, 40])\n",
      "ah transform to a size is: torch.Size([32, 17, 40])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([544, 40, 1])\n",
      "torch.Size([544, 40, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([544, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([544, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([544, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([544, 40, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([544, 40])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([544, 40])\n",
      "ah transform to a size is: torch.Size([32, 17, 40])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([544, 40, 1])\n",
      "torch.Size([544, 40, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([544, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([544, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([544, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 13, 42)\n",
      "first dimension transfer got torch.Size([416, 42])\n",
      "mask dimension transfer got torch.Size([416, 42])\n",
      "last word_vec dimension transfer got torch.Size([416, 42, 768])\n",
      "after drop step dimension transfer got torch.Size([416, 42, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([416, 42, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 13, 42, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([416, 42, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([416, 42, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([416, 42])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([416, 42])\n",
      "ah transform to a size is: torch.Size([32, 13, 42])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([416, 42, 1])\n",
      "torch.Size([416, 42, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([416, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([416, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([416, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([416, 42, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([416, 42])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([416, 42])\n",
      "ah transform to a size is: torch.Size([32, 13, 42])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([416, 42, 1])\n",
      "torch.Size([416, 42, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([416, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([416, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([416, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([416, 42, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([416, 42])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([416, 42])\n",
      "ah transform to a size is: torch.Size([32, 13, 42])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([416, 42, 1])\n",
      "torch.Size([416, 42, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([416, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([416, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([416, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 16, 37)\n",
      "first dimension transfer got torch.Size([512, 37])\n",
      "mask dimension transfer got torch.Size([512, 37])\n",
      "last word_vec dimension transfer got torch.Size([512, 37, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 19/34 [00:03<00:02,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop step dimension transfer got torch.Size([512, 37, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([512, 37, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 16, 37, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([512, 37, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([512, 37, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([512, 37])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([512, 37])\n",
      "ah transform to a size is: torch.Size([32, 16, 37])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([512, 37, 1])\n",
      "torch.Size([512, 37, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([512, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([512, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([512, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([512, 37, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([512, 37])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([512, 37])\n",
      "ah transform to a size is: torch.Size([32, 16, 37])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([512, 37, 1])\n",
      "torch.Size([512, 37, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([512, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([512, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([512, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([512, 37, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([512, 37])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([512, 37])\n",
      "ah transform to a size is: torch.Size([32, 16, 37])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([512, 37, 1])\n",
      "torch.Size([512, 37, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([512, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([512, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([512, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 13, 41)\n",
      "first dimension transfer got torch.Size([416, 41])\n",
      "mask dimension transfer got torch.Size([416, 41])\n",
      "last word_vec dimension transfer got torch.Size([416, 41, 768])\n",
      "after drop step dimension transfer got torch.Size([416, 41, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([416, 41, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 13, 41, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([416, 41, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([416, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([416, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([416, 41])\n",
      "ah transform to a size is: torch.Size([32, 13, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([416, 41, 1])\n",
      "torch.Size([416, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([416, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([416, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([416, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([416, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([416, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([416, 41])\n",
      "ah transform to a size is: torch.Size([32, 13, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([416, 41, 1])\n",
      "torch.Size([416, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([416, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([416, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([416, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([416, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([416, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([416, 41])\n",
      "ah transform to a size is: torch.Size([32, 13, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([416, 41, 1])\n",
      "torch.Size([416, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([416, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([416, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([416, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 15, 40)\n",
      "first dimension transfer got torch.Size([480, 40])\n",
      "mask dimension transfer got torch.Size([480, 40])\n",
      "last word_vec dimension transfer got torch.Size([480, 40, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 21/34 [00:03<00:02,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop step dimension transfer got torch.Size([480, 40, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([480, 40, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 15, 40, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([480, 40, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([480, 40, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([480, 40])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([480, 40])\n",
      "ah transform to a size is: torch.Size([32, 15, 40])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([480, 40, 1])\n",
      "torch.Size([480, 40, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([480, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([480, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([480, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([480, 40, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([480, 40])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([480, 40])\n",
      "ah transform to a size is: torch.Size([32, 15, 40])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([480, 40, 1])\n",
      "torch.Size([480, 40, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([480, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([480, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([480, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([480, 40, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([480, 40])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([480, 40])\n",
      "ah transform to a size is: torch.Size([32, 15, 40])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([480, 40, 1])\n",
      "torch.Size([480, 40, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([480, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([480, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([480, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 9, 38)\n",
      "first dimension transfer got torch.Size([288, 38])\n",
      "mask dimension transfer got torch.Size([288, 38])\n",
      "last word_vec dimension transfer got torch.Size([288, 38, 768])\n",
      "after drop step dimension transfer got torch.Size([288, 38, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([288, 38, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 9, 38, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([288, 38, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([288, 38, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([288, 38])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([288, 38])\n",
      "ah transform to a size is: torch.Size([32, 9, 38])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([288, 38, 1])\n",
      "torch.Size([288, 38, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([288, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([288, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([288, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([288, 38, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([288, 38])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([288, 38])\n",
      "ah transform to a size is: torch.Size([32, 9, 38])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([288, 38, 1])\n",
      "torch.Size([288, 38, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([288, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([288, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([288, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([288, 38, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([288, 38])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([288, 38])\n",
      "ah transform to a size is: torch.Size([32, 9, 38])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([288, 38, 1])\n",
      "torch.Size([288, 38, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([288, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([288, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([288, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 11, 41)\n",
      "first dimension transfer got torch.Size([352, 41])\n",
      "mask dimension transfer got torch.Size([352, 41])\n",
      "last word_vec dimension transfer got torch.Size([352, 41, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 22/34 [00:03<00:01,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after drop step dimension transfer got torch.Size([352, 41, 768])\n",
      "this step got the word level predictions for each sentence ----------------------------------------------------------------------\n",
      "word dimension tanh and linear transfer got, This is Zt torch.Size([352, 41, 22])\n",
      "word dimension transfer back to B, S, T size got, Also another format of Zt torch.Size([32, 11, 41, 22])\n",
      "word-level dimension tanh transfer to value got, this step is getting the key for later calculation torch.Size([352, 41, 768])\n",
      "first dimension chunk transfer got 12 heads of chunks, this is the key in paper (12,)\n",
      "dimension word_key chunk transfer got, function is nn.Parameter.chunk this is the query in paper, this vector has nothing to do with the data (12,)\n",
      "word level encoding done ------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 41])\n",
      "ah transform to a size is: torch.Size([32, 11, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 41, 1])\n",
      "torch.Size([352, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 41])\n",
      "ah transform to a size is: torch.Size([32, 11, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 41, 1])\n",
      "torch.Size([352, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "\n",
      "\n",
      "in the previous key list and query list, each query element size is: torch.Size([352, 41, 64])\n",
      "in the previous key list and query list, each key element size is: torch.Size([64])\n",
      "in the previous attention head, each dot product of ah size is: torch.Size([352, 41])\n",
      "\n",
      "\n",
      "after softmax each attention ah size is: torch.Size([352, 41])\n",
      "ah transform to a size is: torch.Size([32, 11, 41])\n",
      "start the sentence prediction ------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "maxpooling for sentence level prediction P_BSxTxC which is each token level prediction times each head                      prediction, this step is the sumup step in the paper: torch.Size([352, 41, 1])\n",
      "torch.Size([352, 41, 22])\n",
      "after maxpooling the prediction of each attention Zs, its product of tensor (B, S, T, label number) and                      (BxS, S)size is: torch.Size([352, 22])\n",
      "append each sentence prediction to a list, each sentence prediction is stored as a size torch.Size([352, 22])\n",
      "\n",
      "\n",
      "this step perform a token-level likely encoding got h_BSxE size is: torch.Size([352, 64])\n",
      "after the attention transfer, each head prediction was calculated: (12,)\n",
      "current data dimension is (32, 11, 36)\n",
      "first dimension transfer got torch.Size([352, 36])\n",
      "mask dimension transfer got torch.Size([352, 36])\n",
      "last word_vec dimension transfer got torch.Size([352, 36, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3c58c29d1be8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train(dataset_dir, num_aspects, model_name, load_model, train_file, dev_file, data_dir, model_dir, model_type, model_dim, num_heads, \n\u001b[0m\u001b[1;32m      2\u001b[0m       vocab_size, batch_size, learning_rate, no_train_steps, no_warmup_steps, check_every, ckpt_every)\n",
      "\u001b[0;32m<ipython-input-5-7df6a834bd7b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset_dir, num_aspects, model_name, load_model, train_file, dev_file, data_dir, model_dir, model_type, model_dim, num_heads, vocab_size, batch_size, learning_rate, no_train_steps, no_warmup_steps, check_every, ckpt_every)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0minp_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mout_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mdocument_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0msentence_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/untextsum/code/model/s_mil.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_BxSxT, y_true_BxC, p_true_BxSxTxC, step)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdrop_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_warmup_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_warmup_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mdrop_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mdrop_BSxTx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mdrop_BSxTx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_BSxTx1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mdrop_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mx_BSxTxD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_BSxTxD\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdrop_BSxTx1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(dataset_dir, num_aspects, model_name, load_model, train_file, dev_file, data_dir, model_dir, model_type, model_dim, num_heads, \n",
    "      vocab_size, batch_size, learning_rate, no_train_steps, no_warmup_steps, check_every, ckpt_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7350.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "147/0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
